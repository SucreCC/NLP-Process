{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T09:26:06.669002Z",
     "start_time": "2024-12-22T09:26:06.665606Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta"
   ],
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:26:06.679623Z",
     "start_time": "2024-12-22T09:26:06.676768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_CROWTH\"] = \"true\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "b537e1d37b6d45c8",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:26:06.695158Z",
     "start_time": "2024-12-22T09:26:06.691204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PAD, CLS = '[PAD]', '[CLS]'\n",
    "\n",
    "\n",
    "def build_dataset():\n",
    "    def load_dataset(path, pad_size=32):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                content, label = line.split('\\t')\n",
    "                token = tokenizer.tokenize(content)\n",
    "                token = [CLS] + token\n",
    "                seq_len = len(token)\n",
    "                mask = []\n",
    "                token_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "                if pad_size:\n",
    "                    if len(token) < pad_size:\n",
    "                        mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n",
    "                        token_ids += ([0] * (pad_size - len(token)))\n",
    "                    else:\n",
    "                        mask = [1] * pad_size\n",
    "                        token_ids = token_ids[:pad_size]\n",
    "                        seq_len = pad_size\n",
    "                contents.append((token_ids, int(label), seq_len, mask))\n",
    "        return contents\n",
    "\n",
    "    train = load_dataset(train_path, pad_size)\n",
    "    dev = load_dataset(dev_path, pad_size)\n",
    "    test = load_dataset(test_path, pad_size)\n",
    "\n",
    "    return train, dev, test"
   ],
   "id": "8e781b73f8052b67",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:26:06.710626Z",
     "start_time": "2024-12-22T09:26:06.706003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DatasetIterater(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) / batch_size\n",
    "        self.residue = False\n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n",
    "        return (x, seq_len, mask), y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return int(self.n_batches)"
   ],
   "id": "24da7377efce551c",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:26:06.722359Z",
     "start_time": "2024-12-22T09:26:06.720379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_iterator(dataset):\n",
    "    iter = DatasetIterater(dataset, batch_size, device)\n",
    "    return iter\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time.dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time.dif)))"
   ],
   "id": "3cb0878788dfc3df",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:26:09.464478Z",
     "start_time": "2024-12-22T09:26:06.731596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path = './dataset/train.txt'\n",
    "dev_path = './dataset/dev.txt'\n",
    "test_path = './dataset/test.txt'\n",
    "\n",
    "batch_size = 16\n",
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data, dev_data, test_data = build_dataset()\n",
    "train_iter = build_iterator(train_data)\n",
    "dev_iter = build_iterator(dev_data)\n",
    "test_iter = build_iterator(test_data)"
   ],
   "id": "ef2d3d489b6c0ab4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36000it [00:02, 17882.16it/s]\n",
      "2000it [00:00, 19305.77it/s]\n",
      "2000it [00:00, 19160.91it/s]\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:30:16.377340Z",
     "start_time": "2024-12-22T09:30:16.373016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size, model_name):\n",
    "        super(Model, self).__init__()\n",
    "        # 加载预训练的BERT模型\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_cls)\n",
    "\n",
    "        # 设置BERT参数为可训练\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # 最后一层全连接层，将hidden_size映射到单输出\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    # forward方法中，dataset包含 input_ids_list, seq_len, attention_mask\n",
    "    def forward(self, dataset):\n",
    "        input_ids_list = dataset[0]\n",
    "        attention_mask = dataset[2]\n",
    "\n",
    "        # 获取BERT的输出\n",
    "        outputs = self.bert(input_ids=input_ids_list, attention_mask=attention_mask)\n",
    "\n",
    "        # 提取池化后的向量\n",
    "        # pooled_output = outputs.pooler_output\n",
    "\n",
    "        # 通过全连接层后，使用sigmoid激活函数，得到概率值\n",
    "        logits = self.fc(outputs)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs"
   ],
   "id": "ee751f8d4f8c0404",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:29:05.333546Z",
     "start_time": "2024-12-22T09:29:05.325755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "# 训练方法\n",
    "def train(model, train_iter, num_epochs=3, learning_rate=5e-5, accumulate_steps=4):\n",
    "    \"\"\"\n",
    "    训练模型。\n",
    "\n",
    "    参数:\n",
    "    - model: 要训练的模型。\n",
    "    - train_iter: 训练数据迭代器。\n",
    "    - num_epochs: 训练轮数 (默认: 3)。\n",
    "    - learning_rate: 学习率 (默认: 5e-5)。\n",
    "    - accumulate_steps: 梯度累积的步数 (默认: 4)。\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)  # 确保模型在正确的设备上\n",
    "    model.train()  # 设置模型为训练模式\n",
    "\n",
    "    # 优化器和学习率调度器\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "    total_steps = len(train_iter) * num_epochs\n",
    "    warmup_ratio = 0.05\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 训练循环\n",
    "    total_batch = 0  # 记录总的批次\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, num_epochs))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            # 确保训练数据和标签在正确的设备上\n",
    "            # trains, labels = trains.to(device), labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(trains)\n",
    "\n",
    "            # 计算损失（支持梯度累积）\n",
    "            loss = F.binary_cross_entropy(outputs, labels) / accumulate_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度累积更新\n",
    "            if (i + 1) % accumulate_steps == 0 or (i + 1) == len(train_iter):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 梯度裁剪\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # 打印训练信息（每500批次打印一次）\n",
    "            if total_batch % 500 == 0:\n",
    "                time_dif = time.time() - start_time\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>5.2f}, Time: {2}'\n",
    "                print(msg.format(total_batch, loss.item() * accumulate_steps, time_dif))\n",
    "            total_batch += 1\n",
    "\n",
    "    print(\"Training complete!\")"
   ],
   "id": "2ccf6301b1aa8077",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:30:22.052526Z",
     "start_time": "2024-12-22T09:30:21.112169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_size = 768\n",
    "n_cls = 2\n",
    "random_seed = 1221\n",
    "torch.manual_seed(random_seed)\n",
    "pad_size = 32\n",
    "model = Model(n_cls, hidden_size, model_name)\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 3\n",
    "model.to(device)\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)"
   ],
   "id": "f7589771fc6b8150",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:30:24.373530Z",
     "start_time": "2024-12-22T09:30:24.076780Z"
    }
   },
   "cell_type": "code",
   "source": "train(model, train_iter)",
   "id": "eb39a72a0a66cd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[134], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[129], line 51\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_iter, num_epochs, learning_rate, accumulate_steps)\u001B[0m\n\u001B[1;32m     44\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (trains, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_iter):\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;66;03m# 确保训练数据和标签在正确的设备上\u001B[39;00m\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;66;03m# trains, labels = trains.to(device), labels.to(device)\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrains\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# 计算损失（支持梯度累积）\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy(outputs, labels) \u001B[38;5;241m/\u001B[39m accumulate_steps\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[132], line 26\u001B[0m, in \u001B[0;36mModel.forward\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m     20\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(input_ids\u001B[38;5;241m=\u001B[39minput_ids_list, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# 提取池化后的向量\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# pooled_output = outputs.pooler_output\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# 通过全连接层后，使用sigmoid激活函数，得到概率值\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(logits)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m probs\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: linear(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T08:41:32.069986Z",
     "start_time": "2024-12-22T08:41:32.067662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(n_cls, model, test_iter):\n",
    "    # model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(n_cls, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2}, Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n"
   ],
   "id": "df842aae372697ec",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T08:41:32.082909Z",
     "start_time": "2024-12-22T08:41:32.079674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(n_cls, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = []\n",
    "    label_all = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            # # 如果 texts 和 labels 不是张量，先转换为张量\n",
    "            # if not isinstance(texts, torch.Tensor):\n",
    "            #     texts = torch.tensor(texts, dtype=torch.float32)\n",
    "            # if not isinstance(labels, torch.Tensor):\n",
    "\n",
    "            #     labels = torch.tensor(labels, dtype=torch.long)\n",
    "            #\n",
    "            # # 将张量移动到指定设备（如 GPU）\n",
    "            # texts, labels = texts.to(config.device), labels.to(config.device)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss.item()\n",
    "            predic = torch.max(outputs, 1)[1]\n",
    "            predict_all.append(predic)\n",
    "            label_all.append(labels)\n",
    "\n",
    "    # 在 GPU 上拼接结果，并在最后转为 NumPy 数组\n",
    "    predict_all = torch.cat(predict_all).cpu().numpy()\n",
    "    label_all = torch.cat(label_all).cpu().numpy()\n",
    "\n",
    "    acc = metrics.accuracy_score(label_all, predict_all)\n",
    "\n",
    "    if test:\n",
    "        report = metrics.classification_report(label_all, predict_all, target_names=n_cls, digits=4)\n",
    "        confusion = metrics.confusion_matrix(label_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "\n",
    "    return acc, loss_total / len(data_iter)"
   ],
   "id": "3fe216c7365e22d5",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T08:48:27.104148Z",
     "start_time": "2024-12-22T08:48:27.087295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "# 优化后的代码\n",
    "def train(n_cls, model, train_iter, dev_iter, test_iter):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)  # 确保模型在正确的设备上\n",
    "    model.train()  # 调整为只在外层调用一次\n",
    "\n",
    "    start_time = time.time()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "    total_steps = len(train_iter) * num_epochs\n",
    "    warmup_ratio = 0.05\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    total_batch = 0\n",
    "    dev_best_loss = float('inf')\n",
    "    accumulate_steps = 4  # 梯度累积的步数\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, num_epochs))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            # 确保训练数据和标签在正确的设备上\n",
    "            # trains, labels = trains.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(trains)\n",
    "            # time.sleep(0.1)  # time sleep, 减小由GPU利用率过高造成的显示器黑屏问题\n",
    "\n",
    "            loss = F.binary_cross_entropy(outputs, labels) / accumulate_steps  # 累积梯度分摊损失\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulate_steps == 0 or (i + 1) == len(train_iter):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 梯度裁剪\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if total_batch % 500 == 0:\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(n_cls, model, dev_iter)\n",
    "\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    # torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*'\n",
    "                else:\n",
    "                    improve = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>5.2f}, Train Acc: {2:>6.2%}, Val Loss: {3:>5.2f}, Val Acc: {4:>6.2%}, Time: {5}, {6}'\n",
    "                print(\n",
    "                    msg.format(total_batch, loss.item() * accumulate_steps, train_acc, dev_loss, dev_acc, time_dif,\n",
    "                               improve))\n",
    "            total_batch += 1\n",
    "\n",
    "    test(n_cls, model, test_iter)\n"
   ],
   "id": "7a9e1a067c290f77",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T08:41:32.409007Z",
     "start_time": "2024-12-22T08:41:32.104244Z"
    }
   },
   "cell_type": "code",
   "source": "train(n_cls, model, train_iter, dev_iter, test_iter)",
   "id": "23855c42e30fc064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_cls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdev_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_iter\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[55], line 42\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(config, model, train_iter, dev_iter, test_iter)\u001B[0m\n\u001B[1;32m     36\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (trains, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_iter):\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;66;03m# 确保训练数据和标签在正确的设备上\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;66;03m# trains, labels = trains.to(device), labels.to(device)\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrains\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;66;03m# time.sleep(0.1)  # time sleep, 减小由GPU利用率过高造成的显示器黑屏问题\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy(outputs, labels) \u001B[38;5;241m/\u001B[39m accumulate_steps  \u001B[38;5;66;03m# 累积梯度分摊损失\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[50], line 16\u001B[0m, in \u001B[0;36mModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     14\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(input_ids\u001B[38;5;241m=\u001B[39minput_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# pooled_output = outputs.pooler_output\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: linear(): argument 'input' (position 1) must be Tensor, not SequenceClassifierOutput"
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
